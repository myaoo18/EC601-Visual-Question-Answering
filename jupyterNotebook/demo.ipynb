{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2672941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following https://github.com/dukelin95/vqa_pytorch to demo training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7899ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b894ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import data\n",
    "import config\n",
    "import utils\n",
    "import data\n",
    "\n",
    "import model_batchnormDP0\n",
    "import model_batchnormDP05\n",
    "import model_dropout0\n",
    "import model_dropout05\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "from demo.vqaTools.vqa import VQA\n",
    "import random\n",
    "import os\n",
    "from PIL import Image\n",
    "import demo_model\n",
    "import pickle\n",
    "\n",
    "def apply_attention(input, attention):\n",
    "    \"\"\" Apply any number of attention maps over the input. \"\"\"\n",
    "    n, c = input.size()[:2]\n",
    "    glimpses = attention.size(1)\n",
    "\n",
    "    # flatten the spatial dims into the third dim, since we don't need to care about how they are arranged\n",
    "    input = input.view(n, 1, c, -1) # [n, 1, c, s]\n",
    "    attention = attention.view(n, glimpses, -1)\n",
    "    attention = torch.nn.functional.softmax(attention, dim=-1).unsqueeze(2) # [n, g, 1, s]\n",
    "    weighted = attention * input # [n, g, v, s]\n",
    "    weighted_mean = weighted.sum(dim=-1) # [n, g, v]\n",
    "    return weighted_mean.view(n, -1), attention\n",
    "\n",
    "def tens_to_img(image, ax=plt):\n",
    "    image = image.cpu().numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image < 0] = 0\n",
    "    image[image > 1] = 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898213c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load demo weight with RetinaNet_resnet101_50epoch.pth\n",
    "log = torch.load('./RetinaNet_resnet101_50epoch.pth')\n",
    "tokens = len(log['vocab']['question']) + 1\n",
    "net = torch.nn.DataParallel(demo_model.Net(tokens)).cuda()\n",
    "net.load_state_dict(log['weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552daa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load inputs rather than preprocessing the whole dataset, we saved a small subset of the preprocessed inputs\n",
    "v_s, q_s, a_s, q_lens = pickle.load(open(\"demo_inputs.p\", \"rb\"))\n",
    "# load the dictionary for answers\n",
    "ans_voc = pickle.load(open(\"demo_ans_voc_dict.p\", \"rb\"))\n",
    "\n",
    "# hardcoded the answers because the dictionary for questions is too big\n",
    "index_dict = {\n",
    "    5: ['abstract_v002_test2015_000000030017.png', 'how many people are there?'],\n",
    "    10: ['abstract_v002_test2015_000000030033.png', 'what is the person doing?'],\n",
    "    15: ['abstract_v002_test2015_000000030043.png', 'what color is the deer?']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index to 5, 10 or 15, set view_glimpses to True only if you have opencv2\n",
    "index = 5\n",
    "\n",
    "# put to True if you have opencv2\n",
    "view_glimpses = True\n",
    "if view_glimpses:\n",
    "    import cv2\n",
    "\n",
    "imgFilename = index_dict[index][0]\n",
    "question = index_dict[index][1]\n",
    "I = Image.open('demo/' + imgFilename).convert('RGB')\n",
    "\n",
    "if view_glimpses:\n",
    "    # pass image through network\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Scale(config.image_size),\n",
    "                transforms.CenterCrop(config.image_size),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "\n",
    "    I_tens = transform(I) \n",
    "\n",
    "    q = q_s[index]\n",
    "    v = v_s[index]\n",
    "    a = a_s[index]\n",
    "    q_len = q_lens[index]\n",
    "\n",
    "    background = tens_to_img(I_tens)\n",
    "    net.eval()\n",
    "    hq = net.module.text(q, list(q_len.data))\n",
    "    hv = v/(v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "    ha = net.module.attention(hv, hq)\n",
    "    hv, attent = apply_attention(hv, ha)\n",
    "\n",
    "    # get glimpse 1\n",
    "    tens = attent[0, 0, 0, :]\n",
    "    tens.shape[0]\n",
    "    attent_img = tens.view(14,14).to('cuda').detach().numpy()\n",
    "    up_img = ndimage.zoom(attent_img, 32, order=0)\n",
    "    blur = ndimage.gaussian_filter(up_img, sigma=10)\n",
    "    blur3 = 191*blur\n",
    "    overlay = np.stack((blur3,blur3,blur3), axis=2)\n",
    "    glimpse1 = cv2.addWeighted(background, 0.4,overlay,0.1,0)\n",
    "\n",
    "    # get glimpse 2\n",
    "    tens = attent[0, 1, 0, :]\n",
    "    tens.shape[0]\n",
    "    attent_img = tens.view(14,14).to('cuda').detach().numpy()\n",
    "    up_img = ndimage.zoom(attent_img, 32, order=0)\n",
    "    blur = ndimage.gaussian_filter(up_img, sigma=10)\n",
    "    blur3 = 191*blur\n",
    "    overlay = np.stack((blur3,blur3,blur3), axis=2)\n",
    "    glimpse2 = cv2.addWeighted(background, 0.4,overlay,0.1,0)\n",
    "\n",
    "# answers \n",
    "out = net(v, q, q_len)\n",
    "_, answer = out.data.cuda().max(dim=1)\n",
    "answ = (answer.view(-1))\n",
    "\n",
    "print(\"Question: {0}\".format(question))\n",
    "print(\"Network answer: {0}\".format(ans_voc[answ.item()]))\n",
    "f = plt.figure()\n",
    "plt.imshow(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c6fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if view_glimpses:\n",
    "    glimpses= np.concatenate((glimpse1, glimpse2), axis=1)\n",
    "    fig_glimpses = plt.figure(figsize=(7,6))\n",
    "    plt.title(\"Glimpse Maps\")\n",
    "    plt.imshow(glimpses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
